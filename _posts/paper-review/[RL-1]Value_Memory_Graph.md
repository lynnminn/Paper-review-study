# Value Memory Graph: A Graph-Structured word model for offline reinforcement learning

> Editor: [ë¯¼ì˜ˆë¦° (Yerin Min)](https://www.linkedin.com/in/yerinmin/)  
>[![arXiv](https://img.shields.io/badge/arXiv-2206.04384-b31b1b.svg)](https://arxiv.org/pdf/2206.04384.pdf)
>[![Github](https://img.shields.io/badge/GitHub-181717?logo=github&logoColor=white)](https://github.com/TsuTikgiau/ValueMemoryGraph)


## 1. Introduction

- ì‚¬ëŒì´ ì˜ì‚¬ ê²°ì •ì„ í•  ë•ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ì†Œí•œ ì •ë³´ëŠ” ë¬´ì‹œí•˜ê³  ì¤‘ìš”í•œ ì •ë³´ì— ë” ì§‘ì¤‘í•¨ìœ¼ë¡œì¨ ì–´ë ¤ìš´ ë¬¸ì œë¥¼ ë‹¨ìˆœí™”í•¨.
    - ì¼ë°˜ì ìœ¼ë¡œ RL methodsëŠ” original environment ì—ì„œ ì§ì ‘ ìƒí˜¸ì‘ìš©í•˜ë©° policyë¥¼ í•™ìŠµí•¨.
    - ê·¸ëŸ¬ë‚˜ roboticsë‚˜ video gamesê³¼ ê°™ì´ long temporal horizons/sparse reward signal/large and continuous state-action spaceë¥¼ ê°€ì§€ëŠ” ë³µì¡í•œ í™˜ê²½ì—ì„œëŠ” ê¸°ì¡´ RL methodsë¥¼ ì´ìš©í•˜ì—¬ state ë‚˜ actionì˜ ê°€ì¹˜ë¥¼ ì˜ ì¶”ì •í•˜ê³  well-performingí•˜ëŠ” policyë¥¼ ì–»ê¸°ê°€ ì–´ë ¤ì›€.
    - ë³µì¡í•œ original environmentë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•œ **world model**ì„ í•™ìŠµí•˜ì—¬ policy training **ë‚œì´ë„ë¥¼ ë‚®ì¶”ë©´ì„œ ë” ì¢‹ì€ ì„±ëŠ¥**ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŒ.

- ì´ì™€ ê°™ì€ **world model ì´ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” Value Memory Graph(VMG)**ì„.
    - VMGëŠ” íŠ¹íˆ Offline RLì„ ìœ„í•œ graph-structured world modelë¡œ, original environment ë¥¼ ì¶”ìƒí™”í•  ìˆ˜ ìˆëŠ” graph-based MDPë¥¼ ì˜ë¯¸í•¨.
        - Offline RLì€ ë¯¸ë¦¬ ìˆ˜ì§‘ëœ episodes dataset ì„ í™œìš©í•˜ì—¬ policyë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ë¡ ì„.
        - episodeëŠ” original environmentì˜ dynamic informationì„ í¬í•¨í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, offline datasetì„ í™œìš©í•˜ì—¬ offline RL environmentì˜ ì¶”ìƒí™”ë¥¼ ì§ì ‘ í•™ìŠµí•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•¨.
    - VMGë¥¼ ë¨¼ì € í•™ìŠµí•œ í›„ì— RL methodsì— ì ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í™˜ê²½ ëŒ€ì²´ì œë¡œì„œì˜ ê¸°ëŠ¥ì„ í•¨.

- ë³¸ ë°©ë²•ë¡ ì˜ contributionsëŠ” 4ê°€ì§€ê°€ ìˆìŒ.

![overview](./materials/VMG/img_1.png)
1. VMGëŠ” offline RL ì—ì„œì˜ graph-structured world modelì„. VMGëŠ” ìƒíƒœì ìœ¼ë¡œ ì‘ê³  discrete í•œ actionê³¼ state spaceë¥¼ ê°€ì§„ graph-based MDPë¡œ original environmentsë¥¼ í‘œí˜„í•¨.
2. contrastive learning ê³¼ state ë³‘í•©ì„ í†µí•´ offline dataset ì—ì„œ VMGë¥¼ í•™ìŠµí•˜ê³  êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë””ìì¸í•¨.
3. ë³¸ ë…¼ë¬¸ì€ VMG ê¸°ë°˜ì˜ ë°©ë²•ìœ¼ë¡œ value iterationì„ í†µí•´ ë†’ì€ ë¯¸ë˜ ê°€ì¹˜ë¥¼ ê°€ì§€ëŠ” graph actionsì„ ì¶”ë¡ í•˜ê³ , ì´ë¥¼ action translatorë¥¼ í†µí•´ ì‹¤ì œ actionìœ¼ë¡œ ë³€í™˜í•˜ì—¬ agentsë¥¼ controlí•¨.
4. D4RL ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ ê²°ê³¼, VMGëŠ” sparse rewardì™€ long temporal horizonsì„ ê°–ëŠ” ëª‡ëª‡ goal-oriented tasksì— ëŒ€í•´ SOTAë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì„.

[ê´€ë ¨ í‚¤ì›Œë“œ]
ğŸ’¡ #Offline RL 
#Hierarchical RL 
#Model based RL 
#Graph from Experience 
#Representation Learning
<br>

## 3. Value Memory Graph (VMG)

- VMGë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € environment statesê°„ì˜ ë„ë‹¬ ê°€ëŠ¥ì„±(reachability)ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•œ metric space ë¥¼ í•™ìŠµí•¨.
- ê·¸ëŸ° í›„ VMGì˜ backboneìœ¼ë¡œ datasetì˜ metric spaceê°€ graphì— êµ¬ì¶•ëœë‹¤ê³  í•¨.

â†’ ê²°ë¡ ì ìœ¼ë¡œ MDPëŠ” environmentì˜ ì¶”ìƒì ì¸ í‘œí˜„ í˜•íƒœë¡œ(abstract representation) graphì— ì •ì˜ë˜ê²Œ ë¨.

![Untitled](./materials/VMG/img_2.png)

### 3.1 VMG Metric space learning
---

- VMGëŠ” few time stepsì´ ì§„í–‰ëœ í›„ì— ì–´ë–¤ stateê°€ ë‹¤ë¥¸ stateì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” L2 distance ë¥¼ ê¸°ë°˜ì˜ metric space ì—ì„œ êµ¬ì¶•ë˜ì–´ ìˆìŒ.
    - metric space ì˜ embedding ì€ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ Contrastive learning ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•¨.

![Untitled](./materials/VMG/img_3.png)

- ë³¸ ë…¼ë¬¸ì€ 2ê°œì˜ neural networksë¥¼ ì‚¬ìš©í•¨.
    - **state encoder** $E_{nc_s} : S \to f_s$ : original stateë¥¼ metric space ë‚´ state featureë¡œ ë³€í™˜.
    - **action encode**r $E_{nc_a} : f_s,a \to \Delta f_{s,a}$ : original actionì„ metric space ë‚´ transition $\Delta f_{s,a}$ ìœ¼ë¡œ ë³€í™˜.
        - action encoderëŠ” current state feature $f_s$ ì— ë”°ë¼ metric space ë‚´ ì—ì„œ original action aë¥¼  $\Delta f_{s,a}$ë¡œ ë³€í™˜í•¨.
    
    â†’ $\tilde{f_{s'}}=f_s+\Delta f_{s,a}$
    
- State encoder (contrastive loss)
    - ì˜ˆì¸¡ëœ next state $\tilde{f_{s'}}$ê°€ ground truth $f_{s'}$ì™€ ê°€ê¹Œì›Œì§€ê²Œ í•™ìŠµë¨.
        
        ![Untitled](./materials/VMG/img_4.png)
        
        - D: L2 Distance
        - $s_{neg,n}$ : n-th negative state
        - m : margin distance. í•™ìŠµ ë•Œ offline datasetì—ì„œ $(s_i, a_i,s'_i)$ ê°€ randomly sampling ë˜ëŠ”ë°, ì´ë•Œ fixed margin distanceê°€ ì´ìš©ë¨.
            - ëª¨ë“  ë‹¤ë¥¸ next states $s'_{j|j\not=i}$ë¥¼ negative statesë¡œ ì‚¬ìš©í•˜ê³  $\tilde{f_{s'_i}}$ê°€ metric space ì—ì„œ ìµœì†Œ më§Œí¼ ë–¨ì–´ì ¸ ìˆë„ë¡ í•¨.
            
            â†’ ì¦‰ mì„ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡ëœ $\tilde{f_{s'}}$ ê°€ ground truth $f_{s'}$ ì™¸ ë‹¤ë¥¸ statesì™€ëŠ” ìµœì†Œ marin distance ë§Œí¼ ë©€ì–´ì§€ë„ë¡ í•¨. 
            
    
- Action encoder
    - action decoder $Dec_a : f_s, \Delta f_{s,a} \to \tilde{a}$ ë¥¼ í†µí•´ transition $\Delta f_{s,a}$ ë¡œë¶€í„° action reconstruct í•¨.
        - ì´ëŸ° conditioned auto-encoder structureëŠ” $\Delta f_{s,a}$ ê°€ actionì„ ë” ì˜ representation í•˜ë„ë¡ í•¨.
    - ë˜í•œ ì¸ì ‘í•œ statesê°€ metric space ë‚´ì—ì„œ ë”ìš± ê°€ê¹Œì›Œì§€ë„ë¡ í•˜ê¸° ìœ„í•´  transition $\Delta f_{s,a}$ê°€ margin distance m ë³´ë‹¤ í´ ë•Œ í˜ë„í‹°ë¥¼ ë¶€ê³¼í•˜ëŠ” loss ë¥¼ ì¶”ê°€í•¨.
        - . (ì¸ì ‘í•œ ìƒíƒœê°€ ê³„ì† ê°€ê¹Œìš´ ìƒíƒœë¥¼ ìœ ì§€í•´ì•¼ë§Œ agent ì˜ ì´ë™ì„ ì‰½ê²Œ í•œë‹¤ê³  í•˜ëŠ”ë° â€¦)
    
    ![Untitled](./materials/VMG/img_5.png)
    

- total training lossëŠ” ìœ„ ë‘ lossesë¥¼ sum í•œ í˜•íƒœë¡œ ì´ìš©í•¨.
    
    ![Untitled](./materials/VMG/img_6.png)
    
<br>

### 3.2 Construct the graph in VMG
---

![Untitled](./materials/VMG/img_7.png)

- Graph êµ¬ì„± ë°©ë²•
    
    ![Untitled](./materials/VMG/img_8.png)
    
    1. VMGì—ì„œ graphë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € training data ë‚´ì— ìˆëŠ” ëª¨ë“  episodeë¥¼ directed chains í˜•íƒœì¸ metric space ë¡œ ë§µí•‘í•¨.
    2. ê·¸ëŸ° í›„ ìœ„ ê·¸ë¦¼ (a) ì²˜ëŸ¼ state featuresì˜ ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ episode chains ì„ ê²°í•©í•¨.
        - ì´ë•Œ metric space ë‚´ì—ì„œ distance ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ state featureë¥¼ í•˜ë‚˜ì˜ vertexë¡œ ë³‘í•©.
    3. metric space ë‚´ì—ì„œ existing verticesì—ì„œ checking state $s_i$ ê¹Œì§€ì˜ ìµœì†Œ distance ê°€ ì£¼ì–´ì§„ thresh hold $\gamma_m$ ë³´ë‹¤ ì‘ì€ì§€ í™•ì¸í•¨.
        - ë§Œì•½ vertex set ì´ ë¹„ì–´ ìˆë‹¤ë©´ checking state $s_i$ë¥¼ ìƒˆë¡œìš´ vertex $v_J$ë¡œ ì„¤ì •í•˜ê³ , vertex set $V$ì— ì¶”ê°€í•¨.
        - ìœ„ ê³¼ì •ì„ ëª¨ë“  dataset ì— ëŒ€í•´ ë°˜ë³µí•˜ì—¬ vertex ë¥¼ êµ¬ì„±í•¨.
    4. 3ì„ í†µí•´ Vertex set $V$ê°€ êµ¬ì„±ë˜ê³  ë‚˜ë©´, ê°ê°ì˜ state $s_i$ëŠ” metric space ë‚´ì—ì„œ distance ê°€ $\gamma_m$ë³´ë‹¤ ì‘ì€ vertex $v_j$ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆìŒ.
        - training dataset ì—ì„œ each state transition $(s_i,a_i,s'_i)$ëŠ” $s_i$ì—ì„œ $s'_i$ë¡œ ê°€ëŠ” directed connection ìœ¼ë¡œ í‘œí˜„ë¨.
        - ë”°ë¼ì„œ original transition ìœ¼ë¡œë¶€í„° graph directed edgesë¥¼ ìƒì„±í•¨.
<br>

### 3.3 Define a graph-based MDP
---

- graph reward
    
    ![Untitled](./materials/VMG/img_9.png)
<br>    

### 3.4 How to use VMG

---

- VMGëŠ” action translator ë¡œ environment actionsì„ ìƒì„±í•˜ì—¬ episode ì˜ returnsë¥¼ ìµœëŒ€í™”í•˜ëŠ” agent ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒ.
    - classical RL ë°©ë²•ì¸ value iterationì„ í†µí•´ ê° graph state $v_j$ì˜ value $V(v_j)$ë¥¼ ê³„ì‚°í•¨.
        - ì´ë•Œ VMGì˜ finite and discrete state-action spacesë¡œ ì¸í•˜ì—¬, í•™ìŠµí•˜ì§€ ì•Šê³  1ì´ˆ ì´ë‚´ì— value ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤ê³  í•¨.
    - agentê°€ ì¢‹ì€ í–‰ë™ì„ ì·¨í•˜ë„ë¡ VMGëŠ” ê° time stepì—ì„œ high-value graph statesë¡œ ì´ì–´ì§€ëŠ” graph actionì„ ì œê³µí•¨.
        - í•˜ì§€ë§Œ offline dataset ê³¼ environment ê°„ì˜ distribution shift ë¡œ ì¸í•˜ì—¬ VMGì™€ ì‹¤ì œ environment ì‚¬ì´ì—ëŠ” ì°¨ì´ê°€ ì¡´ì¬í•  ìˆ˜ ìˆê³ ,
        - VMGì—ì„œ ì§ì ‘ value iterationì„ í†µí•˜ì—¬ ê³„ì‚°ëœ ìµœì ì˜ graph actionì€ original environmentì—ì„œëŠ” ìµœì ì´ ì•„ë‹ ìˆ˜ë„ ìˆìŒ.
    - ë”°ë¼ì„œ **ë³¸ ë…¼ë¬¸ì˜ ì €ìë“¤ì€** greedy next state valueê°€ í° graph actionsì„ ì„ íƒí•˜ëŠ” ëŒ€ì‹ , **ë¨¼ì € ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ì„œì¹˜í•œ í›„ì— ì¢‹ì€ future stateë¥¼ ì°¾ê³  path ë¥¼ planning í•˜ëŠ” ê²ƒì´ ë” ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì¥í•œë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ë‹¤ê³  í•¨.**

![Untitled](./materials/VMG/img_10.png)

- Policy Execution
    - ì£¼ì–´ì§„ current environment state $s_c$ì—ì„œ ì¸ì ‘í•œ graph state $v_c$ë¥¼ ì°¾ê³ ,
    - $v_c$ë¡œë¶€í„° ì‹œì‘í•˜ëŠ” searchë¥¼ í†µí•´ best value ë¥¼ ê°€ì§€ëŠ” future graph state $v^*$ë¥¼ ì°¾ìŒ.
    - ê·¸ë¦¬ê³  $v_c$ì—ì„œ $v^*$ë¡œ ê°€ëŠ” shorted path $P=[v_c, v_{c+1}, ..., v^*]$ë¥¼ Dijkstra algorithmìœ¼ë¡œ Planning í•¨.
    - $N_{sg}$-th graph state $v_c +N_{sg}$ ë¥¼ ì„ íƒí•˜ê³ , searched graph actionì¸ edge $e_{c \to c+N_{sg}}$ë¥¼ ìƒì„±í•¨.
    - action translatorë¥¼ ì´ìš©í•´ graph action $e_{c \to c+N_{sg}}$ë¥¼ environment action $a_c$ë¡œ ë³€í™˜í•¨.
        - action translatorëŠ” offline dataset ì„ ì´ìš©í•˜ì—¬ surpervised learning ìœ¼ë¡œ í•™ìŠµí•¨.

## 4. Experiments

- D4RL ë²¤ì¹˜ë§ˆí¬ë¥¼ í™œìš©í•œ ì‹¤í—˜

![Untitled](./materials/VMG/img_11.png)
    

## 5. Future works

- í™˜ê²½ êµ¬ì¡°ì˜ ë‹¤ì–‘í•œ ìˆ˜ì¤€ì„ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´, ê³„ì¸µì ì¸ ê·¸ë˜í”„ êµ¬ì¶•ì€ í™˜ê²½ì„ ë” ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ë  ê²ƒ ê°™ìŒ.
    - ì˜ˆë¥¼ ë“¤ì–´, ë¡œë´‡ì´ ìš”ë¦¬ë¥¼ í•  ë•Œ ì•¼ì±„ ì„¸ì²™, ì»·íŒ… ë“± ë‹¨ê³„ë³„ ì‘ì—…ì„ ì§„í–‰í•˜ì•¼ í•˜ëŠ” ê²½ìš° ê³„ì¸¡ì ì¸ êµ¬ì¡°ê°€ ìœ ìš©í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒ.
- ì§€ì†ì ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ í™•ì¥í•˜ê³  ìƒˆë¡œìš´ ì •ë³´ë¥¼ í¬í•¨í•˜ê¸° ìœ„í•œ ë©”ì¹´ë‹ˆì¦˜ì´ í•„ìš”í•¨.
- íƒìƒ‰ì„ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•´ MCTSë¥¼ ì‚¬ìš©í•˜ë©´ ë„ì›€ì´ ë  ê²ƒ ê°™ìŒ.

---

## Appendix

![Untitled](./materials/VMG/img_12.png)

- hyper-params setting
    
    ![Untitled](./materials/VMG/img_13.png)
    

- influence of margin distance
    
    ![Untitled](./materials/VMG/img_14.png)