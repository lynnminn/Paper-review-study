{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Active Retrieval Augmented Generation\n",
    "\n",
    "## Overview\n",
    "- RAG (Retrieval-Augment Generation) 구조, 일반 RAG 구조와 다르게 **생성과정**에서 언제, 무엇을 검색할지 정함\n",
    "- 한계\n",
    "    - 모델의 능력(CoT, 판단 능력) 등이 좋아야하는 것으로 보임\n",
    "    - 또한 임계점 방식을 사용하는데 이를 위해서 데이터셋마다 추가 실험이 필요함\n",
    "    - 일반적인 QA short-answer에 대해서도 성능이 좋아졌는지에 관한 실험은 진행하지 않음.\n",
    "\n",
    "<img src=\"./img/FLARE/Architecture.png\" alt=\"FLARE 구조\" width=\"1000\" >\n",
    "\n",
    "## Background\n",
    "## 기본 RAG(Retrieval-Augment Generation) 구조\n",
    "- <img src=\"./img/FLARE/RAG.png\" widht=800>\n",
    "\n",
    "- 간단하게 말하자면 LLM에 사용자의 질문을 넣어서 답을 구할때 사용자의 질문만 넣는 것이 아니라 관련된 document를 검색해서 generate 하는 방식\n",
    "- 자세한 과정\n",
    "    1) Query가 들어오면 Query 인코더를 통과해 임베딩을 뽑음\n",
    "    2) 임베딩 내적값을 활용해 query와 가장 유사한 document를 검색\n",
    "    3) 그 후 검색된 document(Top-k) + Query를 LLM generator(보통 Bart, T5) r에 input으로 넣음\n",
    "    4) LLM (generator) 가 답변을 generate\n",
    "\n",
    "\n",
    "## Benchmark\n",
    "### Single-time Retrieval Augmented Generation\n",
    "- 흔한 RAG방식 한번만 검색해서 바로 응답\n",
    "- **단점**: 정보를 한번만 검색에 사용하기 때문에 Long-generation task에 부적합…\n",
    "\n",
    "### Previous window(Retro)\n",
    "- 질문 임베딩을 청크로 나누어서 검색해서 사용\n",
    "- Cross-attention을 청크와 검색문서사이에 진행, 추후에 합치는…\n",
    "\n",
    "### Previous Sentence (IR CoT)\n",
    "- Retrieval과 Reasoning(CoT)를 같이 활용\n",
    "- 마지막 CoT 문장을 query로 해서 검색에 활용하면서 최종 답안을 내는 형태\n",
    "- So the answer이 나올때까지 계속 같은 단계 수행\n",
    "- **단점**: 이전에 생성된 토큰을 기준으로 검색을 진행하기때문에 LLM이 향후 생성하고자 하는걸 반영 X\n",
    "\n",
    "### Question-Decomposition(Self-ASK)\n",
    "<img src=\"./img/FLARE/self-ask.png\" width=800>\n",
    "\n",
    "- 질문을 분해해서 Multi-hop 질문을 할 수 있도록…\n",
    "- **단점**: Problem (task)에 따라서 대응 /  prompt engineering 을 진행해야함\n",
    "    - 예시 수학문제, 시멘트 파싱 등등\n",
    "\n",
    "\n",
    "## Method\n",
    "- “생성될” 문장을 활용해서 지속적으로 정보를 재검색하면서 재 생성\n",
    "- 즉 generation 하면서 정보가 필요하면 검색하고 아니면 자기의 내제된 지식으로 답변하는 느낌.\n",
    "\n",
    "### FLARE: Forward-Looking Active REtrieval Augmented Generation\n",
    "\n",
    "#### 1. FLARE with Retrieval Instructions\n",
    "- LLM이 생성하다가 추가 정보가 필요할때마다 [Search(query)] 생성\n",
    "- prompt 예시<br>\n",
    "   <img src=\"./img/FLARE/FLARE_with_Retrieval_Instructions.png\">\n",
    "\n",
    "#### 2. Direct FLARE\n",
    "동기\n",
    "- 앞에 방식 1번은 LLM의 능력을 그대로 믿는 방법 → 생성한 Query를 신뢰 할 수 없다는 단점이 존재\n",
    "- 따라서 원래 LLM이 잘하는 **다음문장을** 가지고 결정하자!!!\n",
    "\n",
    "과정\n",
    "- LLM은 time t 마다 다음 문장을 생성함 (검색 문서 없이, 원래 LLM처럼 다음문장 예측)\n",
    "    - $\\hat{s_t} = LM(x,y_{<t})$\n",
    "- 이 $\\hat{s_t}$ 을 가지고 검색을 trigger를 할지 정하고, 질문 queries를 생성함\n",
    "- LLM이 $\\hat{s_t}$ 에 대한 confidence를 가지고 있다면, 추가 검색을 하지 않고 계속 이어서 생성\n",
    "그렇지 않다면, 검색을 진행, 검색된 문서를 같이 활용해서 $\\hat{s_t}$  생성\n",
    "- 마지막 <eos> 토큰이 등장할때까지 이걸 반복\n",
    "\n",
    "Confidence 계산 방법\n",
    "- well-calibrated 특징 사용\n",
    "    - LLM이 지식이 부족할때는 낮은 확률을 뱉는 경향이 있음\n",
    "- 따라서 생성한 모든 토큰이 임계값을 넘는지 체크\n",
    "    - 넘으면 LLM이 지식에 대한 확신이 있다(외부지식이 필요 없다) 하면 바로 사용\n",
    "    - 그렇지 않다면 검색 활용\n",
    "\n",
    "검색 Query 생성 방법\n",
    "- 이것도 임계값 기준으로 ($\\beta$) <br>\n",
    "    <img src=\"./img/FLARE/make_query.png\">\n",
    "1) Implicit query\n",
    "- 토큰들 중 임계값보다 낮은 토큰을 마스킹하고 검색에 활용\n",
    "- 계산 식 <br>\n",
    "   <img src=\"./img/FLARE/implict_query.png\">\n",
    "\n",
    "2) explicit query\n",
    "- 토큰들 중 임계값보다 낮은 토큰과 llm을 활용해서 새롭게 query를 만들어서 검색에 활용\n",
    "- prompt 예시 <br>\n",
    "   <img src=\"./img/FLARE/explict_query.png\">\n",
    "\n",
    "## Experiment\n",
    "<img src=\"./img/FLARE/main_result.png\">\n",
    "\n",
    "- 모든 benchmark에서 성능이 개선됨.\n",
    "- MultiHopQA에서 가장 크게 개선\n",
    "    - 2-hop reasoning process를 목적으로 만들어진 벤치마크로 최종답변을 비슷하게 생성할수 있기 때문에\n",
    "- ASQA, WikiAsp는 비교적 적게 개선됨.\n",
    "    - 답변이 비교적으로 길기때문에 생성에 어려움\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
